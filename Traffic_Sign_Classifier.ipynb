{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, Add, Lambda, Cropping2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Activation, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "ADAPT_LEARNING_RATE = False\n",
    "DISABLE_PRINTS = False\n",
    "EVALUATE_BATCH_TRAINING = True\n",
    "AUGMENT_DATA = True\n",
    "TRAIN_MODEL = True\n",
    "GRAY_SCALE = False\n",
    "LEARNING_CURVE = False # If False trains for all the training set\n",
    "\n",
    "# Load pickled data\n",
    "training_file = './traffic-signs-data/train.p'\n",
    "\n",
    "validation_file= './traffic-signs-data/valid.p'\n",
    "testing_file = './traffic-signs-data/test.p'\n",
    "\n",
    "train_dict = pickle.load(open(training_file, mode='rb') )\n",
    "valid_dict = pickle.load(open(validation_file, mode='rb'))\n",
    "test_dict  = pickle.load(open(testing_file, mode='rb'))\n",
    "\n",
    "coords_train, X_train, sizes_train, y_train = train_dict['coords'], train_dict['features'], train_dict['sizes'], train_dict['labels']\n",
    "\n",
    "X_valid, y_valid = valid_dict['features'], valid_dict['labels']\n",
    "X_test, y_test = test_dict['features'], test_dict['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import createCLAHE\n",
    "\n",
    "def isGrayScale(image):\n",
    "    if image.shape[-1] == 3 and len(image.shape) == 3:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def histogram_equalize_image(image, gridSize = (4, 4)):\n",
    "    clahe = createCLAHE(tileGridSize=gridSize)\n",
    "    if isGrayScale(image): \n",
    "        image = clahe.apply(image)            \n",
    "        return image\n",
    "    else:\n",
    "        img_YCrCb = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)            \n",
    "        img_YCrCb[...,0] = clahe.apply(img_YCrCb[...,0])            \n",
    "        image = cv2.cvtColor(img_YCrCb, cv2.COLOR_YCrCb2RGB)\n",
    "        return image\n",
    "\n",
    "def normalize_image(image):\n",
    "    # pixels after normalization are in range [-0.5, +0.5]\n",
    "    return (image - np.min(image))/(np.max(image)-np.min(image)) - 0.5\n",
    "\n",
    "def preprocess_image(image, grayscale = GRAY_SCALE):  \n",
    "    if grayscale:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)       \n",
    "    return normalize_image(histogram_equalize_image(image))\n",
    "\n",
    "def preprocess_data(X_data):\n",
    "    return np.array([preprocess_image(image) for image in X_data]).reshape(X_data.shape[0], 32,32,-1)\n",
    "\n",
    "\n",
    "def augment_data(X_data, enable = False):\n",
    "    return np.array([random_transform_image(image, enable) for image in X_data]).reshape(X_data.shape[0], 32,32,-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_images(images, xaxis = \"\", yaxis =\"\", cmap=None, ncols = 10, title = ''):\n",
    "    \"\"\"\n",
    "    Display a list of images in a single figure with matplotlib.\n",
    "        Parameters:\n",
    "            images: An np.array compatible with plt.imshow.\n",
    "            yaxis (Default = \" \"): A string to be used as a label for each image.\n",
    "            cmap (Default = None): Used to display gray images.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))    \n",
    "    n_images = len(images)\n",
    "    nrows = np.ceil(n_images/ncols) \n",
    "\n",
    "    for i in range(n_images):\n",
    "        plt.subplot(nrows, ncols, i+1)\n",
    "        #Use gray scale color map if there is only one channel\n",
    "        cmap = 'gray' if images[i].shape[-1] != 3 else cmap\n",
    "        plt.imshow(np.squeeze(images[i]), cmap = cmap)\n",
    "        plt.xlabel(xaxis)\n",
    "        plt.ylabel(yaxis)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])    \n",
    "    plt.tight_layout(pad=0, h_pad=0, w_pad=0)    \n",
    "    plt.title(title, loc = 'center', fontsize=16)        \n",
    "    plt.show()        \n",
    "\n",
    "    \n",
    "def rotate_image(image, angle_range = 30):\n",
    "    rows,cols,ch = image.shape\n",
    "    random_angle = np.random.uniform(angle_range)-angle_range/2    \n",
    "    rot_mat = cv2.getRotationMatrix2D((cols/2,rows/2), random_angle, scale = 1)\n",
    "    return cv2.warpAffine(image,rot_mat,(cols,rows))\n",
    "\n",
    "def translate_image(image, trans_range = 5):\n",
    "    rows,cols,ch = image.shape\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "    return cv2.warpAffine(image,Trans_M,(cols,rows))\n",
    "\n",
    "def shear_image(image, shear_range = 5):\n",
    "    rows,cols,ch = image.shape\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "    return cv2.warpAffine(image,shear_M,(cols,rows))\n",
    "\n",
    "def change_brightness_image(image, brightness_range = 0.5):  \n",
    "    random_bright = 1 + brightness_range*np.random.uniform() - brightness_range/2 \n",
    "    if not isGrayScale(image):\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_RGB2YCrCb)        \n",
    "        image[:,:,0] = image[:,:,0]*random_bright\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_YCrCb2RGB)    \n",
    "        return image\n",
    "    else:        \n",
    "        image = image*random_bright       \n",
    "        return image\n",
    "\n",
    "def motion_blur_image(image):\n",
    "    size = 3\n",
    "    # generating the kernel\n",
    "    kernel_motion_blur = np.zeros((size, size))\n",
    "    kernel_motion_blur[int((size-1)/2), :] = np.ones(size)\n",
    "    kernel_motion_blur = kernel_motion_blur / size\n",
    "    return cv2.filter2D(image, -1, kernel_motion_blur)\n",
    "    \n",
    "def random_transform_image(image, enable = False):    \n",
    "    id_transformation = np.random.randint(2)\n",
    "    if not enable or id_transformation == 0:\n",
    "        return image\n",
    "    \n",
    "    rotation_flag = np.random.randint(2)\n",
    "    if rotation_flag == 1:\n",
    "        image = rotate_image(image)\n",
    "        \n",
    "    translation_flag = np.random.randint(2)\n",
    "    if translation_flag == 1:\n",
    "        image = translate_image(image)\n",
    "    \n",
    "    shear_flag = np.random.randint(2)\n",
    "    if shear_flag == 1:\n",
    "        image = shear_image(image)\n",
    "\n",
    "    brightness_flag = np.random.randint(2)\n",
    "    if brightness_flag == 1:\n",
    "        image = change_brightness_image(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data):\n",
    "    # ********************** hyper-parameters ************************\n",
    "    batch_size  = 1024 # non consequential parameter for eval\n",
    "    # ********************** **********************************************\n",
    "    \n",
    "    num_examples    = X_data.shape[0]\n",
    "    totalAccuracy   = 0\n",
    "    totalLoss       = 0\n",
    "\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        end = np.minimum(offset + batch_size, num_examples)\n",
    "        batch_x, batch_y = X_data[offset:end], y_data[offset:end]\n",
    "        batch_accuracy, batch_loss = sess.run([accuracy_operation, loss_operation], \n",
    "                                              feed_dict={x: batch_x, \n",
    "                                                           y: batch_y, \n",
    "                                                           BN: False, \n",
    "                                                           keep_prob_fc: np.ones(2).reshape([2, 1]),\n",
    "                                                           beta_regularization: np.zeros(2).reshape([2, 1])})\n",
    "        totalAccuracy   += (batch_accuracy * batch_x.shape[0])\n",
    "        totalLoss       += (batch_loss * batch_x.shape[0])\n",
    "\n",
    "    return {\"loss\": totalLoss/num_examples, \"accuracy\": totalAccuracy/num_examples}\n",
    "\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_valid, y_valid, hyperPar_dict):    \n",
    "    # ********************** read hyper-parameters ************************\n",
    "    epochs              = hyperPar_dict['epochs']\n",
    "    batch_size          = hyperPar_dict['batch_size']\n",
    "    learning_rate       = hyperPar_dict['learning_rate']\n",
    "    batch_norm          = hyperPar_dict['batch_norm']\n",
    "    keep_probability_fc = hyperPar_dict['keep_prob_fc']\n",
    "    beta_regularization_parameters = hyperPar_dict['beta_regularization']\n",
    "    # ********************** ********************* ************************\n",
    "    \n",
    "    training_loss       = np.zeros(epochs)\n",
    "    training_accuracy   = np.zeros(epochs)\n",
    "    validation_loss     = np.zeros(epochs)\n",
    "    validation_accuracy = np.zeros(epochs)    \n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:               \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        max_validation_accuracy = 0.95\n",
    "        for i in range(epochs): \n",
    "            t1 = time.time()\n",
    "            if (i+1)%20 == 0 and ADAPT_LEARNING_RATE:\n",
    "                learning_rate *= 0.3\n",
    "            \n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "            for offset in range(0, X_train.shape[0], batch_size):\n",
    "                end = np.minimum(offset + batch_size, X_train.shape[0])\n",
    "                batch_x, batch_y = preprocess_data(augment_data(X_train[offset:end], enable = AUGMENT_DATA)), y_train[offset:end]\n",
    "                sess.run(training_operation, feed_dict={x: batch_x, \n",
    "                                                        y: batch_y, \n",
    "                                                        BN: batch_norm, \n",
    "                                                        learn_rate: learning_rate,\n",
    "                                                        keep_prob_fc: keep_probability_fc,\n",
    "                                                        beta_regularization: beta_regularization_parameters})\n",
    "\n",
    "            t2 = time.time()            \n",
    "            # **************** update results after an epoch ************************\n",
    "            if EVALUATE_BATCH_TRAINING:\n",
    "                training_results_dict   = evaluate(preprocess_data(X_train[offset:end]), y_train[offset:end])\n",
    "            else:\n",
    "                training_results_dict   = evaluate(preprocess_data(X_train), y_train)\n",
    "\n",
    "            validation_results_dict = evaluate(X_valid, y_valid)\n",
    "            t3 = time.time()\n",
    "\n",
    "            training_accuracy[i]    = training_results_dict['accuracy']\n",
    "            training_loss[i]        = training_results_dict['loss']        \n",
    "            validation_accuracy[i]  = validation_results_dict['accuracy']\n",
    "            if validation_accuracy[i]  > max_validation_accuracy:\n",
    "                max_validation_accuracy = validation_accuracy[i]\n",
    "                saver.save(sess, './OptimizedModel')\n",
    "            validation_loss[i]      = validation_results_dict['loss']        \n",
    "            # *********************************************************************** \n",
    "            if not DISABLE_PRINTS:            \n",
    "                print(\"EPOCH {} ...\".format(i+1))\n",
    "                print(\"Time taken = {:.2f}, {:.2f} \".format((t2-t1), (t3-t2)))\n",
    "                print(\"Learning rate = {:.3f}\".format(learning_rate*1000))\n",
    "                print(\"Training Accuracy = {:.3f}\".format(training_accuracy[i]))\n",
    "                print(\"Training Loss = {:.3f}\".format(training_loss[i]))\n",
    "                print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy[i]))\n",
    "                print(\"Validation Loss = {:.3f}\".format(validation_loss[i]))\n",
    "                print()\n",
    "            \n",
    "        \n",
    "            \n",
    "    \n",
    "    results_dict = {\"training_accuracy\": training_accuracy,\n",
    "        \"validation_accuracy\": validation_accuracy,\n",
    "        \"training_loss\": training_loss,\n",
    "        \"validation_loss\": validation_loss}\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "def learning_curve(X_train, y_train, X_valid, y_valid, hyperPar_dict, step_size = 2048):\n",
    "    training_accuracy_list      = []\n",
    "    training_loss_list          = []\n",
    "    validation_accuracy_list    = []    \n",
    "    validation_loss_list        = []\n",
    "    \n",
    "    step_size = np.minimum(step_size, X_train.shape[0])\n",
    "    \n",
    "    \n",
    "    for m_train in range(step_size, X_train.shape[0]+1, step_size):            \n",
    "        m_valid = int(np.minimum(np.round(m_train*.15), X_valid.shape[0]))\n",
    "        print(\"Training using {training_samples} training samples and {valid_samples} validation samples\"\n",
    "              .format(training_samples = m_train, valid_samples = m_valid))\n",
    "        print()\n",
    "\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        X_valid, y_valid = shuffle(X_valid, y_valid)        \n",
    "        #*******************************************\n",
    "        # Train the model for m samples\n",
    "        # ******************************************\n",
    "        results_dict_mSamples = train_model(X_train[0:m_train], y_train[0:m_train],\n",
    "        X_valid[0:m_valid], y_valid[0:m_valid], \n",
    "        hyperPar_dict)            \n",
    "\n",
    "        training_accuracy_list.append(results_dict_mSamples['training_accuracy'])\n",
    "        training_loss_list.append(results_dict_mSamples['training_loss'])\n",
    "\n",
    "        validation_accuracy_list.append(results_dict_mSamples['validation_accuracy'])\n",
    "        validation_loss_list.append(results_dict_mSamples['validation_loss'])\n",
    "        \n",
    "\n",
    "    learning_curve_results = {\n",
    "        \"training_accuracy\": np.array(training_accuracy_list),\n",
    "        \"training_loss\": np.array(training_loss_list),\n",
    "        \"validation_accuracy\": np.array(validation_accuracy_list),\n",
    "        \"validation_loss\": np.array(validation_loss_list)\n",
    "    }   \n",
    "    \n",
    "\n",
    "    return learning_curve_results\n",
    "\n",
    "\n",
    "def display_stats(X_train, y_train, X_valid, y_valid):\n",
    "    # Number of training examples\n",
    "    n_train = X_train.shape[0]\n",
    "\n",
    "    # Number of validation examples\n",
    "    n_valid = X_valid.shape[0]\n",
    "\n",
    "    # Number of testing examples.\n",
    "    n_test = X_test.shape[0]\n",
    "\n",
    "    # shape of an traffic sign image\n",
    "    image_shape = np.array([X_train.shape[1], X_train.shape[2]])\n",
    "\n",
    "    # How many unique classes/labels there are in the dataset.\n",
    "    N_CLASSES = len(np.unique(y_train))\n",
    "\n",
    "    # bin count of different classes\n",
    "    n_train_images_in_a_Class = np.bincount(y_train)\n",
    "\n",
    "    print(\"Number of training examples =\", n_train)\n",
    "    print(\"Number of validation examples =\", n_valid)\n",
    "    print(\"Number of testing examples =\", n_test)\n",
    "    print(\"Image data shape =\", image_shape)\n",
    "    print(\"Number of classes =\", N_CLASSES)\n",
    "\n",
    "    # Mapping ClassID to traffic sign names\n",
    "    signs = []\n",
    "    signnames = csv.reader(open('signnames.csv', 'r'), delimiter=',')\n",
    "    next(signnames,None)\n",
    "    for row in signnames:\n",
    "        signs.append(row[1])\n",
    "    \n",
    "\n",
    "def display_samples(X_data, coords = None, sizes = None):\n",
    "    # Display sample images\n",
    "    disp_sample_idx  = np.random.randint(0, X_data.shape[0], 10)\n",
    "\n",
    "    # display original samples\n",
    "    print('original images')\n",
    "    list_images(X_data[disp_sample_idx])\n",
    "\n",
    "    print('boxed original images')\n",
    "    list_images(get_boxed_images(data_set   = X_data[disp_sample_idx], coords = coords[disp_sample_idx], sizes = sizes[disp_sample_idx]))\n",
    "    print('Cropped images')\n",
    "    list_images(get_cropped_images(data_set = X_data[disp_sample_idx], coords = coords[disp_sample_idx], sizes = sizes[disp_sample_idx]))\n",
    "\n",
    "    # display normalized samples\n",
    "    print('normalized images')\n",
    "    list_images(normalize_dataset(X_data[disp_sample_idx]))\n",
    "\n",
    "    print('histogram equalized gray scale images')\n",
    "    list_images(histogram_equalization(X_data[disp_sample_idx]))\n",
    "\n",
    "\n",
    "\n",
    "def explore_data(X_train, y_train, X_valid, y_valid):\n",
    "    # explore data set for random 1000 images\n",
    "    sample_id  = np.random.randint(0, X_train.shape[0], 1000)\n",
    "    # Mean and variance image of a data set\n",
    "    X_explore_data = X_train[sample_id]\n",
    "\n",
    "    mean_img = np.mean(X_explore_data, axis = 0).astype('uint8')\n",
    "    std_img = np.std(X_explore_data, axis = 0).astype('uint8')\n",
    "    list_images([mean_img, std_img])\n",
    "    # Compute histogram of entropy\n",
    "    # pixel_variance(data_set, equalize = False)\n",
    "    variance_data = histogram_equalization(X_explore_data, equalize = False)\n",
    "    plt.hist(variance_data)\n",
    "    plt.show()\n",
    "\n",
    "    # Histogram of train\n",
    "    hist_y_train = np.bincount(y_train)\n",
    "    hist_y_valid = np.bincount(y_valid)\n",
    "\n",
    "    nhist_y_train = hist_y_train\n",
    "    nhist_y_valid = hist_y_valid\n",
    "\n",
    "    y_pos = np.arange(len(np.unique(y_train)))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(y_pos, nhist_y_train, 0.35, align='center', alpha=0.5, color = 'b', label = 'train')\n",
    "    #ax.bar(y_pos, nhist_y_valid, 0.8, align='center', alpha=0.5, color = 'g', label = 'valid')\n",
    "    ax.set_ylabel('count of images in a class')\n",
    "    ax.set_xlabel('class ID')\n",
    "    legend = ax.legend(loc='upper center', shadow=True, fontsize='x-large')\n",
    "    plt.show() \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    #ax.bar(y_pos, nhist_y_train, 0.35, align='center', alpha=0.5, color = 'b', label = 'train')\n",
    "    ax.bar(y_pos, nhist_y_valid, 0.8, align='center', alpha=0.5, color = 'g', label = 'valid')\n",
    "    ax.set_ylabel('count of images in a class')\n",
    "    ax.set_xlabel('class ID')\n",
    "    legend = ax.legend(loc='upper center', shadow=True, fontsize='x-large')\n",
    "    plt.show() \n",
    "\n",
    "    print(\"Distribution of different classes in training is \\n\", (hist_y_train))     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Neural network architecture (is the network over or underfitting?)\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten, batch_norm\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')\n",
    "\n",
    "def activation(x, func = 'relu', BN = False):\n",
    "    if BN == True:\n",
    "        if func == 'relu':\n",
    "            return batch_norm(tf.nn.relu(x)) \n",
    "        elif func == 'elu':\n",
    "            return batch_norm(tf.nn.elu(x))\n",
    "        elif func == 'lrelu':\n",
    "            return batch_norm(tf.nn.leaky_relu(x, alpha = 0.1))\n",
    "        else:\n",
    "            assert False, \" unsupported activation function used\"\n",
    "    else:\n",
    "        if func == 'relu':\n",
    "            return tf.nn.relu(x)\n",
    "        elif func == 'elu':\n",
    "            return tf.nn.elu(x)\n",
    "        elif func == 'lrelu':\n",
    "            return tf.nn.leaky_relu(x, alpha = 0.1)\n",
    "        else:\n",
    "            assert False, \" unsupported activation function used\"\n",
    "    \n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='VALID') + b\n",
    "    \n",
    "\n",
    "# tf.constant(value,dtype=None)    \n",
    "def LeNet(x, batch_norm = False, keep_prob_fc = np.ones(2).reshape([2 , 1]),\n",
    "         beta_regularization = np.zeros(2).reshape([2 , 1])):   \n",
    "    \n",
    "    BN_flag = batch_norm\n",
    "    activation_func = 'relu'    \n",
    "\n",
    "    if not GRAY_SCALE:\n",
    "        # conv00: Input = 32x32X3. Output = 32x32x3.\n",
    "        # General color map transformation\n",
    "        wc00 = tf.get_variable(\"wc00\", shape = [1, 1, 3, 3], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        bc00 = tf.Variable(tf.zeros(3))\n",
    "        conv00 = conv2d(x, wc00, bc00)\n",
    "        #layer00_out = activation(conv00, func = activation_func, BN = BN_flag)\n",
    "        layer00_out = conv00\n",
    "\n",
    "        # conv01: Input = 32x32X3. Output = 32x32x1.\n",
    "        # General gray scale transformation\n",
    "        wc01 = tf.get_variable(\"wc01\", shape = [1, 1, 3, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        bc01 = tf.Variable(tf.zeros(1))\n",
    "        conv01 = conv2d(layer00_out, wc01, bc01)\n",
    "        #layer01_out = activation(conv01, func = activation_func, BN = BN_flag)\n",
    "        layer01_out = conv01\n",
    "    else:\n",
    "        layer01_out = x\n",
    "        \n",
    "    # conv1: Input = 32x32X1. Output = 28x28x6.\n",
    "    # Weights = 5X5X1X6 = 150\n",
    "    wc1 = tf.get_variable(\"wc1\", shape = [5, 5, 1, 6], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    bc1 = tf.Variable(tf.zeros(6))\n",
    "    conv1 = conv2d(layer01_out, wc1, bc1)\n",
    "    #Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    maxp1 = maxpool2d(conv1, k=2)\n",
    "    #Activation. Output = 14x14x6.\n",
    "    layer1_out = activation(maxp1, func = activation_func, BN = BN_flag)\n",
    "\n",
    "    \n",
    "    # conv2: Input = 14x14X6. Output = 10x10x16.\n",
    "    # Weights = 5X5X6X16 = 2.4k\n",
    "    wc2 = tf.get_variable(\"wc2\", shape = [5, 5, 6, 16], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    bc2 = tf.Variable(tf.zeros(16))\n",
    "    conv2 = conv2d(layer1_out, wc2, bc2)\n",
    "    #Pooling. Input = 10x10x16.. Output = 5x5x16..\n",
    "    maxp2 = maxpool2d(conv2, k=2)\n",
    "    #Activation. Output = 5x5x16.\n",
    "    layer2_out = activation(maxp2, func = activation_func, BN = BN_flag)\n",
    "\n",
    "    \n",
    "    # Flatten. Input = 5x5x16. Output = 400\n",
    "    fc3_inp = flatten(layer2_out)\n",
    "\n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    # Weights = 400*120 = 48k\n",
    "    wfc3 = tf.get_variable(\"wfc3\", shape = [400, 120], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    bfc3 = tf.Variable(tf.zeros(120))    \n",
    "    # Sameer: change tf.add and tf.multiply\n",
    "    fc3  = tf.add(tf.matmul(fc3_inp,wfc3),bfc3)\n",
    "    # TODO: Activation.\n",
    "    layer3_out = activation(fc3, func = activation_func, BN = BN_flag)\n",
    "    layer3_out = tf.nn.dropout(layer3_out, keep_prob_fc[0,0])\n",
    "\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    # Weights = 120*84 ~ 10k\n",
    "    wfc4 = tf.get_variable(\"wfc4\", shape = [120, 84], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    bfc4 = tf.Variable(tf.zeros(84))    \n",
    "    fc4  = tf.add(tf.matmul(layer3_out, wfc4),bfc4)\n",
    "    # TODO: Activation.\n",
    "    layer4_out = activation(fc4, func = activation_func, BN = BN_flag)\n",
    "    layer4_out = tf.nn.dropout(layer4_out, keep_prob_fc[1,0])\n",
    "    \n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    # Weights = 84*43 ~ 3.6k\n",
    "    # Biases = 43\n",
    "    wfc5 = tf.get_variable(\"wfc5\", shape = [84, 43], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    bfc5 = tf.Variable(tf.zeros(43))    \n",
    "    fc5  = tf.add(tf.matmul(layer4_out, wfc5),bfc5)\n",
    "    logits = fc5\n",
    "   \n",
    "\n",
    "    regularizers = beta_regularization[0,0] * tf.nn.l2_loss(wfc3) + beta_regularization[1,0]*tf.nn.l2_loss(wfc4)\n",
    "    return logits, regularizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-f5154cc61024>:39: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if GRAY_SCALE:\n",
    "    n_channels = 1\n",
    "else:\n",
    "    n_channels = 3\n",
    "\n",
    "\n",
    "#************************************************************************************\n",
    "#                               INPUT-Variables\n",
    "#************************************************************************************\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, n_channels))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "# Regularization\n",
    "keep_prob_fc = tf.placeholder(tf.float32, shape = (2, 1))\n",
    "beta_regularization = tf.placeholder(tf.float32, shape = (2, 1))\n",
    "# Learning rate\n",
    "learn_rate = tf.placeholder(tf.float32, (None))\n",
    "# Batch norm\n",
    "BN = tf.placeholder(tf.bool, (None))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N_CLASSES = 43\n",
    "one_hot_y = tf.one_hot(y, N_CLASSES)\n",
    "\n",
    "\n",
    "#************************************************************************************\n",
    "#                               Choose the neural network model\n",
    "#************************************************************************************\n",
    "logits, regularizers = LeNet(x, batch_norm = BN, keep_prob_fc = keep_prob_fc, beta_regularization = beta_regularization)\n",
    "saver = tf.train.Saver()\n",
    "#************************************************************************************\n",
    "\n",
    "\n",
    "#************************************************************************************\n",
    "#               Define compute nodes of interest in a given graph\n",
    "#************************************************************************************\n",
    "cross_entropy  = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(tf.reduce_mean(cross_entropy) + regularizers)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learn_rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "prediction = tf.argmax(logits, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using 34799 training samples and 4410 validation samples\n",
      "\n",
      "EPOCH 1 ...\n",
      "Time taken = 24.75, 0.98 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.660\n",
      "Training Loss = 1.393\n",
      "Validation Accuracy = 0.532\n",
      "Validation Loss = 1.530\n",
      "\n",
      "EPOCH 2 ...\n",
      "Time taken = 27.81, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.851\n",
      "Training Loss = 0.523\n",
      "Validation Accuracy = 0.719\n",
      "Validation Loss = 0.872\n",
      "\n",
      "EPOCH 3 ...\n",
      "Time taken = 27.93, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.894\n",
      "Training Loss = 0.327\n",
      "Validation Accuracy = 0.802\n",
      "Validation Loss = 0.616\n",
      "\n",
      "EPOCH 4 ...\n",
      "Time taken = 28.07, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.872\n",
      "Training Loss = 0.394\n",
      "Validation Accuracy = 0.830\n",
      "Validation Loss = 0.500\n",
      "\n",
      "EPOCH 5 ...\n",
      "Time taken = 28.25, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.936\n",
      "Training Loss = 0.177\n",
      "Validation Accuracy = 0.867\n",
      "Validation Loss = 0.423\n",
      "\n",
      "EPOCH 6 ...\n",
      "Time taken = 28.16, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.957\n",
      "Training Loss = 0.177\n",
      "Validation Accuracy = 0.878\n",
      "Validation Loss = 0.365\n",
      "\n",
      "EPOCH 7 ...\n",
      "Time taken = 28.07, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.894\n",
      "Training Loss = 0.168\n",
      "Validation Accuracy = 0.902\n",
      "Validation Loss = 0.294\n",
      "\n",
      "EPOCH 8 ...\n",
      "Time taken = 28.19, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.110\n",
      "Validation Accuracy = 0.919\n",
      "Validation Loss = 0.258\n",
      "\n",
      "EPOCH 9 ...\n",
      "Time taken = 28.48, 1.16 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.957\n",
      "Training Loss = 0.127\n",
      "Validation Accuracy = 0.922\n",
      "Validation Loss = 0.259\n",
      "\n",
      "EPOCH 10 ...\n",
      "Time taken = 27.99, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.056\n",
      "Validation Accuracy = 0.933\n",
      "Validation Loss = 0.235\n",
      "\n",
      "EPOCH 11 ...\n",
      "Time taken = 27.97, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.117\n",
      "Validation Accuracy = 0.933\n",
      "Validation Loss = 0.222\n",
      "\n",
      "EPOCH 12 ...\n",
      "Time taken = 28.14, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.034\n",
      "Validation Accuracy = 0.940\n",
      "Validation Loss = 0.207\n",
      "\n",
      "EPOCH 13 ...\n",
      "Time taken = 28.36, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.068\n",
      "Validation Accuracy = 0.950\n",
      "Validation Loss = 0.180\n",
      "\n",
      "EPOCH 14 ...\n",
      "Time taken = 28.07, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.052\n",
      "Validation Accuracy = 0.952\n",
      "Validation Loss = 0.181\n",
      "\n",
      "EPOCH 15 ...\n",
      "Time taken = 28.12, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.042\n",
      "Validation Accuracy = 0.953\n",
      "Validation Loss = 0.176\n",
      "\n",
      "EPOCH 16 ...\n",
      "Time taken = 28.22, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.024\n",
      "Validation Accuracy = 0.957\n",
      "Validation Loss = 0.165\n",
      "\n",
      "EPOCH 17 ...\n",
      "Time taken = 28.18, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.048\n",
      "Validation Accuracy = 0.955\n",
      "Validation Loss = 0.170\n",
      "\n",
      "EPOCH 18 ...\n",
      "Time taken = 28.05, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.033\n",
      "Validation Accuracy = 0.961\n",
      "Validation Loss = 0.157\n",
      "\n",
      "EPOCH 19 ...\n",
      "Time taken = 28.27, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.059\n",
      "Validation Accuracy = 0.964\n",
      "Validation Loss = 0.141\n",
      "\n",
      "EPOCH 20 ...\n",
      "Time taken = 28.34, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.033\n",
      "Validation Accuracy = 0.968\n",
      "Validation Loss = 0.141\n",
      "\n",
      "EPOCH 21 ...\n",
      "Time taken = 28.26, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.083\n",
      "Validation Accuracy = 0.965\n",
      "Validation Loss = 0.143\n",
      "\n",
      "EPOCH 22 ...\n",
      "Time taken = 28.07, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.026\n",
      "Validation Accuracy = 0.962\n",
      "Validation Loss = 0.143\n",
      "\n",
      "EPOCH 23 ...\n",
      "Time taken = 28.08, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.011\n",
      "Validation Accuracy = 0.962\n",
      "Validation Loss = 0.151\n",
      "\n",
      "EPOCH 24 ...\n",
      "Time taken = 28.22, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.029\n",
      "Validation Accuracy = 0.966\n",
      "Validation Loss = 0.147\n",
      "\n",
      "EPOCH 25 ...\n",
      "Time taken = 30.73, 1.17 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.020\n",
      "Validation Accuracy = 0.965\n",
      "Validation Loss = 0.138\n",
      "\n",
      "EPOCH 26 ...\n",
      "Time taken = 29.95, 1.37 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.049\n",
      "Validation Accuracy = 0.966\n",
      "Validation Loss = 0.148\n",
      "\n",
      "EPOCH 27 ...\n",
      "Time taken = 30.61, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.006\n",
      "Validation Accuracy = 0.965\n",
      "Validation Loss = 0.121\n",
      "\n",
      "EPOCH 28 ...\n",
      "Time taken = 29.25, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.972\n",
      "Validation Loss = 0.121\n",
      "\n",
      "EPOCH 29 ...\n",
      "Time taken = 28.67, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.069\n",
      "Validation Accuracy = 0.972\n",
      "Validation Loss = 0.109\n",
      "\n",
      "EPOCH 30 ...\n",
      "Time taken = 28.08, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.020\n",
      "Validation Accuracy = 0.972\n",
      "Validation Loss = 0.116\n",
      "\n",
      "EPOCH 31 ...\n",
      "Time taken = 28.13, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.011\n",
      "Validation Accuracy = 0.970\n",
      "Validation Loss = 0.121\n",
      "\n",
      "EPOCH 32 ...\n",
      "Time taken = 28.27, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.125\n",
      "Validation Accuracy = 0.969\n",
      "Validation Loss = 0.133\n",
      "\n",
      "EPOCH 33 ...\n",
      "Time taken = 28.21, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.021\n",
      "Validation Accuracy = 0.972\n",
      "Validation Loss = 0.112\n",
      "\n",
      "EPOCH 34 ...\n",
      "Time taken = 28.05, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.013\n",
      "Validation Accuracy = 0.970\n",
      "Validation Loss = 0.116\n",
      "\n",
      "EPOCH 35 ...\n",
      "Time taken = 28.05, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.024\n",
      "Validation Accuracy = 0.973\n",
      "Validation Loss = 0.094\n",
      "\n",
      "EPOCH 36 ...\n",
      "Time taken = 28.29, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.006\n",
      "Validation Accuracy = 0.974\n",
      "Validation Loss = 0.097\n",
      "\n",
      "EPOCH 37 ...\n",
      "Time taken = 28.32, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.035\n",
      "Validation Accuracy = 0.972\n",
      "Validation Loss = 0.090\n",
      "\n",
      "EPOCH 38 ...\n",
      "Time taken = 28.04, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.010\n",
      "Validation Accuracy = 0.975\n",
      "Validation Loss = 0.096\n",
      "\n",
      "EPOCH 39 ...\n",
      "Time taken = 28.22, 1.18 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.003\n",
      "Validation Accuracy = 0.975\n",
      "Validation Loss = 0.092\n",
      "\n",
      "EPOCH 40 ...\n",
      "Time taken = 28.28, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.008\n",
      "Validation Accuracy = 0.973\n",
      "Validation Loss = 0.102\n",
      "\n",
      "EPOCH 41 ...\n",
      "Time taken = 28.16, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.014\n",
      "Validation Accuracy = 0.969\n",
      "Validation Loss = 0.104\n",
      "\n",
      "EPOCH 42 ...\n",
      "Time taken = 28.08, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.008\n",
      "Validation Accuracy = 0.973\n",
      "Validation Loss = 0.104\n",
      "\n",
      "EPOCH 43 ...\n",
      "Time taken = 28.05, 1.17 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.004\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.091\n",
      "\n",
      "EPOCH 44 ...\n",
      "Time taken = 28.32, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.012\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.091\n",
      "\n",
      "EPOCH 45 ...\n",
      "Time taken = 28.12, 1.16 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.023\n",
      "Validation Accuracy = 0.973\n",
      "Validation Loss = 0.098\n",
      "\n",
      "EPOCH 46 ...\n",
      "Time taken = 28.19, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.003\n",
      "Validation Accuracy = 0.971\n",
      "Validation Loss = 0.100\n",
      "\n",
      "EPOCH 47 ...\n",
      "Time taken = 28.05, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.008\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.093\n",
      "\n",
      "EPOCH 48 ...\n",
      "Time taken = 28.40, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.033\n",
      "Validation Accuracy = 0.975\n",
      "Validation Loss = 0.095\n",
      "\n",
      "EPOCH 49 ...\n",
      "Time taken = 28.08, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.003\n",
      "Validation Accuracy = 0.975\n",
      "Validation Loss = 0.090\n",
      "\n",
      "EPOCH 50 ...\n",
      "Time taken = 28.45, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.008\n",
      "Validation Accuracy = 0.974\n",
      "Validation Loss = 0.105\n",
      "\n",
      "EPOCH 51 ...\n",
      "Time taken = 28.06, 1.16 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.037\n",
      "Validation Accuracy = 0.974\n",
      "Validation Loss = 0.092\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 52 ...\n",
      "Time taken = 28.10, 1.27 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.024\n",
      "Validation Accuracy = 0.974\n",
      "Validation Loss = 0.098\n",
      "\n",
      "EPOCH 53 ...\n",
      "Time taken = 28.09, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.053\n",
      "Validation Accuracy = 0.975\n",
      "Validation Loss = 0.091\n",
      "\n",
      "EPOCH 54 ...\n",
      "Time taken = 28.26, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.004\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.087\n",
      "\n",
      "EPOCH 55 ...\n",
      "Time taken = 28.17, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.974\n",
      "Validation Loss = 0.084\n",
      "\n",
      "EPOCH 56 ...\n",
      "Time taken = 28.30, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.009\n",
      "Validation Accuracy = 0.980\n",
      "Validation Loss = 0.083\n",
      "\n",
      "EPOCH 57 ...\n",
      "Time taken = 28.34, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.029\n",
      "Validation Accuracy = 0.975\n",
      "Validation Loss = 0.096\n",
      "\n",
      "EPOCH 58 ...\n",
      "Time taken = 28.27, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.041\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.082\n",
      "\n",
      "EPOCH 59 ...\n",
      "Time taken = 28.07, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.043\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.082\n",
      "\n",
      "EPOCH 60 ...\n",
      "Time taken = 28.32, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.006\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.084\n",
      "\n",
      "EPOCH 61 ...\n",
      "Time taken = 28.24, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.028\n",
      "Validation Accuracy = 0.975\n",
      "Validation Loss = 0.088\n",
      "\n",
      "EPOCH 62 ...\n",
      "Time taken = 28.25, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.016\n",
      "Validation Accuracy = 0.974\n",
      "Validation Loss = 0.095\n",
      "\n",
      "EPOCH 63 ...\n",
      "Time taken = 28.10, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.004\n",
      "Validation Accuracy = 0.972\n",
      "Validation Loss = 0.094\n",
      "\n",
      "EPOCH 64 ...\n",
      "Time taken = 28.16, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.011\n",
      "Validation Accuracy = 0.979\n",
      "Validation Loss = 0.077\n",
      "\n",
      "EPOCH 65 ...\n",
      "Time taken = 28.39, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.027\n",
      "Validation Accuracy = 0.973\n",
      "Validation Loss = 0.094\n",
      "\n",
      "EPOCH 66 ...\n",
      "Time taken = 28.20, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.008\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.081\n",
      "\n",
      "EPOCH 67 ...\n",
      "Time taken = 28.09, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.011\n",
      "Validation Accuracy = 0.974\n",
      "Validation Loss = 0.094\n",
      "\n",
      "EPOCH 68 ...\n",
      "Time taken = 28.22, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.017\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.081\n",
      "\n",
      "EPOCH 69 ...\n",
      "Time taken = 28.37, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.972\n",
      "Validation Loss = 0.093\n",
      "\n",
      "EPOCH 70 ...\n",
      "Time taken = 28.53, 1.16 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.012\n",
      "Validation Accuracy = 0.974\n",
      "Validation Loss = 0.093\n",
      "\n",
      "EPOCH 71 ...\n",
      "Time taken = 28.27, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.009\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.092\n",
      "\n",
      "EPOCH 72 ...\n",
      "Time taken = 28.13, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.084\n",
      "\n",
      "EPOCH 73 ...\n",
      "Time taken = 28.35, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.019\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.081\n",
      "\n",
      "EPOCH 74 ...\n",
      "Time taken = 28.24, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.008\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.084\n",
      "\n",
      "EPOCH 75 ...\n",
      "Time taken = 28.11, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.122\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.077\n",
      "\n",
      "EPOCH 76 ...\n",
      "Time taken = 28.14, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.016\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.072\n",
      "\n",
      "EPOCH 77 ...\n",
      "Time taken = 28.39, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.094\n",
      "\n",
      "EPOCH 78 ...\n",
      "Time taken = 28.27, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.004\n",
      "Validation Accuracy = 0.975\n",
      "Validation Loss = 0.089\n",
      "\n",
      "EPOCH 79 ...\n",
      "Time taken = 28.13, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.980\n",
      "Validation Loss = 0.075\n",
      "\n",
      "EPOCH 80 ...\n",
      "Time taken = 28.25, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.979\n",
      "Validation Loss = 0.072\n",
      "\n",
      "EPOCH 81 ...\n",
      "Time taken = 28.33, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.040\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.083\n",
      "\n",
      "EPOCH 82 ...\n",
      "Time taken = 28.18, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.004\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.079\n",
      "\n",
      "EPOCH 83 ...\n",
      "Time taken = 28.08, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.084\n",
      "\n",
      "EPOCH 84 ...\n",
      "Time taken = 28.12, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.005\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.086\n",
      "\n",
      "EPOCH 85 ...\n",
      "Time taken = 28.24, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.980\n",
      "Validation Loss = 0.077\n",
      "\n",
      "EPOCH 86 ...\n",
      "Time taken = 28.37, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.017\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.083\n",
      "\n",
      "EPOCH 87 ...\n",
      "Time taken = 28.12, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.089\n",
      "\n",
      "EPOCH 88 ...\n",
      "Time taken = 28.21, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.006\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.072\n",
      "\n",
      "EPOCH 89 ...\n",
      "Time taken = 29.88, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.060\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.072\n",
      "\n",
      "EPOCH 90 ...\n",
      "Time taken = 28.11, 1.16 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.975\n",
      "Validation Loss = 0.072\n",
      "\n",
      "EPOCH 91 ...\n",
      "Time taken = 28.20, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.029\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.078\n",
      "\n",
      "EPOCH 92 ...\n",
      "Time taken = 28.02, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.974\n",
      "Validation Loss = 0.083\n",
      "\n",
      "EPOCH 93 ...\n",
      "Time taken = 28.07, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.071\n",
      "\n",
      "EPOCH 94 ...\n",
      "Time taken = 27.98, 1.18 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.034\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.078\n",
      "\n",
      "EPOCH 95 ...\n",
      "Time taken = 28.14, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.000\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.069\n",
      "\n",
      "EPOCH 96 ...\n",
      "Time taken = 28.00, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.004\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.088\n",
      "\n",
      "EPOCH 97 ...\n",
      "Time taken = 28.11, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.079\n",
      "\n",
      "EPOCH 98 ...\n",
      "Time taken = 28.00, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.006\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.078\n",
      "\n",
      "EPOCH 99 ...\n",
      "Time taken = 28.09, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.004\n",
      "Validation Accuracy = 0.974\n",
      "Validation Loss = 0.087\n",
      "\n",
      "EPOCH 100 ...\n",
      "Time taken = 27.98, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.005\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.069\n",
      "\n",
      "EPOCH 101 ...\n",
      "Time taken = 28.15, 1.19 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.038\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.068\n",
      "\n",
      "EPOCH 102 ...\n",
      "Time taken = 28.00, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.980\n",
      "Validation Loss = 0.073\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 103 ...\n",
      "Time taken = 28.17, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.980\n",
      "Validation Loss = 0.065\n",
      "\n",
      "EPOCH 104 ...\n",
      "Time taken = 28.01, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.005\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.074\n",
      "\n",
      "EPOCH 105 ...\n",
      "Time taken = 28.00, 1.14 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.979\n",
      "Validation Loss = 0.075\n",
      "\n",
      "EPOCH 106 ...\n",
      "Time taken = 28.26, 1.15 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.079\n",
      "\n",
      "EPOCH 107 ...\n",
      "Time taken = 28.08, 1.16 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.003\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.075\n",
      "\n",
      "EPOCH 108 ...\n",
      "Time taken = 27.95, 1.18 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.089\n",
      "\n",
      "EPOCH 109 ...\n",
      "Time taken = 28.67, 1.17 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.070\n",
      "\n",
      "EPOCH 110 ...\n",
      "Time taken = 28.75, 1.17 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.081\n",
      "\n",
      "EPOCH 111 ...\n",
      "Time taken = 33.25, 1.19 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.025\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.078\n",
      "\n",
      "EPOCH 112 ...\n",
      "Time taken = 32.54, 1.50 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.005\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.080\n",
      "\n",
      "EPOCH 113 ...\n",
      "Time taken = 40.07, 1.20 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.005\n",
      "Validation Accuracy = 0.977\n",
      "Validation Loss = 0.077\n",
      "\n",
      "EPOCH 114 ...\n",
      "Time taken = 35.90, 1.18 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 0.979\n",
      "Training Loss = 0.044\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.085\n",
      "\n",
      "EPOCH 115 ...\n",
      "Time taken = 30.20, 1.17 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.075\n",
      "\n",
      "EPOCH 116 ...\n",
      "Time taken = 32.02, 1.40 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.979\n",
      "Validation Loss = 0.069\n",
      "\n",
      "EPOCH 117 ...\n",
      "Time taken = 31.95, 1.26 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.001\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.076\n",
      "\n",
      "EPOCH 118 ...\n",
      "Time taken = 31.05, 1.23 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.978\n",
      "Validation Loss = 0.083\n",
      "\n",
      "EPOCH 119 ...\n",
      "Time taken = 31.93, 1.34 \n",
      "Learning rate = 1.000\n",
      "Training Accuracy = 1.000\n",
      "Training Loss = 0.002\n",
      "Validation Accuracy = 0.976\n",
      "Validation Loss = 0.097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#X_train = preprocess_data(train_dict['features'])\n",
    "X_valid = preprocess_data(valid_dict['features'])\n",
    "X_test  = preprocess_data(test_dict['features'])\n",
    "\n",
    "if not LEARNING_CURVE:\n",
    "    step_size = X_train.shape[0]\n",
    "\n",
    "# ********************** Set hyper-parameters ************************\n",
    "hyperPar_dict = {\"epochs\": 200,\n",
    "\"batch_size\": 64,\n",
    "\"learning_rate\": 1e-3,\n",
    "\"batch_norm\": True,\n",
    "\"keep_prob_fc\": np.array([0.5, 0.5]).reshape([2, 1]),\n",
    "\"beta_regularization\": np.array([0, 0]).reshape([2, 1])}\n",
    "# ********************** ********************* ************************\n",
    "    \n",
    "learning_curve_results = learning_curve(X_train, y_train, X_valid, y_valid, hyperPar_dict, step_size)\n",
    "\n",
    "training_accuracy_vs_m = learning_curve_results['training_accuracy'][:,-1]\n",
    "training_loss_vs_m = learning_curve_results ['training_loss'][:,-1]\n",
    "validation_accuracy_vs_m = learning_curve_results['validation_accuracy'][:,-1]\n",
    "validation_loss_vs_m = learning_curve_results['validation_loss'][:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,  './OptimizedModel')\n",
    "    train_results_dict   = evaluate(preprocess_data(X_train), y_train)\n",
    "    print(\"Train Accuracy = {:.3f}\".format(train_results_dict['accuracy']))\n",
    "    print(\"Train Loss = {:.3f}\".format(train_results_dict['loss']))\n",
    "    \n",
    "    valid_results_dict   = evaluate(X_valid, y_valid)\n",
    "    print(\"Validation Accuracy = {:.3f}\".format(valid_results_dict['accuracy']))\n",
    "    print(\"Validation Loss = {:.3f}\".format(valid_results_dict['loss']))\n",
    "    \n",
    "    test_results_dict   = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_results_dict['accuracy']))\n",
    "    print(\"Test Loss = {:.3f}\".format(test_results_dict['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy_vs_iter = learning_curve_results['training_accuracy'][0,:]\n",
    "training_loss_vs_iter = learning_curve_results['training_loss'][0,:]\n",
    "validation_accuracy_vs_iter = learning_curve_results['validation_accuracy'][0,:]\n",
    "validation_loss_vs_iter = learning_curve_results['validation_loss'][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "num = 0\n",
    "plt.figure()\n",
    "for i in range(learning_curve_results['training_accuracy'].shape[0]):\n",
    "    if i==num:\n",
    "        training_loss_vs_iter = learning_curve_results['training_loss'][i,:]\n",
    "        validation_loss_vs_iter = learning_curve_results['validation_loss'][i,:]    \n",
    "        plt.plot(np.arange(1, len(training_loss_vs_iter)+1), training_loss_vs_iter, label = 'train')\n",
    "        plt.plot(np.arange(1, len(validation_loss_vs_iter)+1), validation_loss_vs_iter, label = 'valid')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "legend = plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "for i in range(learning_curve_results['training_accuracy'].shape[0]):\n",
    "    if i==num:\n",
    "        training_accuracy_vs_iter = learning_curve_results['training_accuracy'][i,:]\n",
    "        validation_accuracy_vs_iter = learning_curve_results['validation_accuracy'][i,:]    \n",
    "        plt.plot(np.arange(1, len(training_accuracy_vs_iter)+1), training_accuracy_vs_iter, label = 'train')\n",
    "        plt.plot(np.arange(1, len(validation_accuracy_vs_iter)+1), validation_accuracy_vs_iter, label = 'valid')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "legend = plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, len(training_accuracy_vs_iter)+1), training_accuracy_vs_iter, 'b', label = 'training accuracy')\n",
    "plt.plot(np.arange(1, len(validation_accuracy_vs_iter)+1), validation_accuracy_vs_iter, 'r', label = 'validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "legend = plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(validation_accuracy_vs_iter))\n",
    "print(np.max(validation_accuracy_vs_iter))\n",
    "print(np.argmax(training_accuracy_vs_iter))\n",
    "print(np.max(training_accuracy_vs_iter))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
